{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><img src=\"https://oceanprotocol.com/static/media/banner-ocean-03@2x.b7272597.png\" alt=\"drawing\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Ocean Protocol - Manta Ray project</center></h1>\n",
    "<h3><center>Decentralized Data Science and Engineering, powered by Ocean Protocol</center></h3>\n",
    "<p>Version 0.5.1 - beta</p>\n",
    "<p>Package compatibility: squid-py v0.5.11, keeper-contracts 0.9.0, utilities 0.2.1,\n",
    "<p>Component compatibility: Brizo v0.3.2, Aquarius v0.2.1, Nile testnet smart contracts 0.8.6</p>\n",
    "<p><a href=\"https://github.com/oceanprotocol/mantaray\">mantaray on Github</a></p>\n",
    "<p>\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decentralized Data Science use case - Understanding the Amazon deforestation from Space challenge\n",
    "\n",
    "This notebook demonstrates accessing a dataset from Ocean Protocol and training a deep learning classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"https://www.borgenmagazine.com/wp-content/uploads/2013/11/Deforestation.jpg\" alt=\"drawing\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "Attribution: Source for this notebook was prepared by Tuatini Godard for the Kaggle Competition \"Planet: Understanding the Amazon from Space\"\n",
    "\n",
    "See the [source kernel](https://www.kaggle.com/ekami66/0-92837-on-private-lb-solution-with-keras)\n",
    "\n",
    "And the [source GitHub repository](https://github.com/EKami/planet-amazon-deforestation)\n",
    "\n",
    "Modifications and refactoring made by M.Jones 17 May 2019\n",
    "- Data source paths updated\n",
    "- New plotting function\n",
    "- Various formatting and comments\n",
    "\n",
    "Below, the kernel;\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Special thanks to the kernel contributors of this challenge (especially @anokas and @Kaggoo) who helped me find a starting point for this notebook.\n",
    "\n",
    "The whole code including the `data_helper.py` and `keras_helper.py` files are available on github [here](https://github.com/EKami/planet-amazon-deforestation) and the notebook can be found on the same github [here](https://github.com/EKami/planet-amazon-deforestation/blob/master/notebooks/amazon_forest_notebook.ipynb)\n",
    "\n",
    "**If you found this notebook useful some upvotes would be greatly appreciated! :) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Prepare environment and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "==========================================================================="
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "# =============================================================================\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "FORMAT = \"%(levelno)-2s %(asctime)s : %(message)s\"\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Paths"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Move to the project directory if inside a subdirectory (JupyterLab)\n",
    "import os\n",
    "cwd = Path.cwd().parts\n",
    "if cwd[-1] == 'kernel_submission':\n",
    "    cwd = cwd[0:-1]\n",
    "    cwd = Path(*cwd)\n",
    "    os.chdir(cwd)\n",
    "    logging.info(\"Changed working directory to {}\".format(cwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "from matplotlib.pyplot import imshow\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "\n",
    "logging.info(\"{:>10}=={} as {}\".format('numpy', np.__version__, 'np'))\n",
    "logging.info(\"{:>10}=={} as {}\".format('pandas', pd.__version__, 'pd'))\n",
    "logging.info(\"{:>10}=={} as {}\".format('sklearn', sk.__version__, 'sk'))\n",
    "logging.info(\"{:>10}=={} as {}\".format('seaborn', sns.__version__, 'sns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "assert \"LD_LIBRARY_PATH\" in os.environ\n",
    "# Deep learning stack\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks\n",
    "logging.info(\"{:>10}=={} as {}\".format('tensorflow', tf.__version__, 'tf'))\n",
    "logging.info(\"{:>10}=={} as {}\".format('keras', ks.__version__, 'ks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# import gc\n",
    "# import bcolz\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, History\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, History\n",
    "# import vgg16\n",
    "# from utils import vgg16\n",
    "# from . import utils\n",
    "# from .. import utils\n",
    "# import .utils\n",
    "# from  .utils import vgg16\n",
    "# from utils import vgg16\n",
    "# import vgg16\n",
    "# import data_helper\n",
    "\n",
    "# import data_helper\n",
    "from src.utils import vgg16\n",
    "try:\n",
    "    from src.utils import vgg16\n",
    "    from src.utils import data_helper\n",
    "    from src.utils.data_helper import AmazonPreprocessor\n",
    "except:\n",
    "    path_utils = Path.cwd() / '../src'\n",
    "    assert path_utils.exists()\n",
    "    sys.path.insert(0, str(path_utils.resolve()))\n",
    "    from utils import vgg16\n",
    "    from utils import data_helper\n",
    "    from utils.data_helper import AmazonPreprocessor\n",
    "# from src.utils import vgg16\n",
    "# from src.utils import data_helper\n",
    "# from src.utils.data_helper import AmazonPreprocessor\n",
    "# from kaggle_data.downloader import KaggleDataDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def mm2inch(value):\n",
    "    return value/25.4\n",
    "PAPER = {\n",
    "    \"A3_LANDSCAPE\" : (mm2inch(420),mm2inch(297)),\n",
    "    \"A4_LANDSCAPE\" : (mm2inch(297),mm2inch(210)),\n",
    "    \"A5_LANDSCAPE\" : (mm2inch(210),mm2inch(148)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Access and download the data from Ocean Protocol\n",
    "Data asset Decentralized Identifier (did): `did:op:3fdcc402b9994d88828e82f9be16e40eaf8eed10036c48ae9a826415e3ca46ce`\n",
    "\n",
    "Commons market link: [Amazon rainforest satellite imagery](https://commons.oceanprotocol.com/asset/did:op:3fdcc402b9994d88828e82f9be16e40eaf8eed10036c48ae9a826415e3ca46ce)\n",
    "\n",
    "Optionally, download the data directly in your notebook environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import logging\n",
    "import os\n",
    "from squid_py import Metadata, Ocean\n",
    "import squid_py\n",
    "import mantaray_utilities as manta_utils\n",
    "\n",
    "# Setup logging\n",
    "from mantaray_utilities.user import get_account_from_config\n",
    "from mantaray_utilities.blockchain import subscribe_event\n",
    "manta_utils.logging.logger.setLevel('INFO')\n",
    "import mantaray_utilities as manta_utils\n",
    "from squid_py import Config\n",
    "from squid_py.keeper import Keeper\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import web3\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data path is relative to current path\n",
    "from pathlib import Path\n",
    "PATH_DATA_ROOT = Path.cwd() / \"data\"\n",
    "if not PATH_DATA_ROOT.exists():\n",
    "    PATH_DATA_ROOT = Path.cwd() / \"../data\"\n",
    "assert PATH_DATA_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_folder = Path.cwd() / 'data'\n",
    "assert data_root_folder.exists()\n",
    "train_jpeg_dir = data_root_folder / 'train-jpg'\n",
    "assert train_jpeg_dir.exists()\n",
    "test_jpeg_dir = data_root_folder / 'test-jpg'\n",
    "assert test_jpeg_dir.exists()\n",
    "test_jpeg_additional_dir = data_root_folder / 'test-jpg-additional'\n",
    "assert test_jpeg_additional_dir.exists()\n",
    "train_csv_file = data_root_folder / 'train_v2.csv'\n",
    "assert train_csv_file.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Section 2: Basic data exploration and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inspect image labels\n",
    "Visualize what the training set looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_jpeg_dir, test_jpeg_dir, test_jpeg_additional, train_csv_file = data_helper.get_jpeg_data_files_paths()\n",
    "labels_df = pd.read_csv(train_csv_file)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each image can be tagged with multiple tags, lets list all uniques tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print all unique tags\n",
    "from itertools import chain\n",
    "labels_list = list(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values]))\n",
    "labels_set = set(labels_list)\n",
    "print(\"{} labels: {}\".format(len(labels_set), labels_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition of each labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# Histogram of label instances\n",
    "labels_s = pd.Series(labels_list).value_counts() # To sort them by count\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.barplot(x=labels_s, y=labels_s.index, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Images\n",
    "Visualize some chip images to know what we are dealing with.\n",
    "Lets vizualise 1 chip for the 17 images to get a sense of their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_title = [labels_df[labels_df['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg'\n",
    "                for i, label in enumerate(labels_set)]\n",
    "\n",
    "plt.rc('axes', grid=False)\n",
    "#cols = 3\n",
    "#rows = 3\n",
    "_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, (image_name, label) in enumerate(zip(images_title, labels_set)):\n",
    "    img = mpimg.imread(train_jpeg_dir / image_name)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title('{} - {}'.format(image_name, label))\n",
    "plt.show()# %% [markdown]\n",
    "# ## Section 3: Pre-process images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Image resize & validation split\n",
    "Define the dimensions of the image data trained by the network. Recommended resized images could be 32x32, 64x64, or 128x128 to speedup the training.\n",
    "\n",
    "You could also use `None` to use full sized images.\n",
    "\n",
    "Be careful, the higher the `validation_split_size` the more RAM you will consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_resize = (128, 128) # The resize size of each image ex: (64, 64) or None to use the default image size\n",
    "validation_split_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preprocessing\n",
    "Due to the huge amount of memory the preprocessed images can take, we will create a dedicated `AmazonPreprocessor` class which job is to preprocess the data right in time at specific steps (training/inference) so that our RAM don't get completely filled by the preprocessed images.\n",
    "\n",
    "The only exception to this being the validation dataset as we need to use it as-is for f2 score calculation as well as when we calculate the validation accuracy of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = AmazonPreprocessor(train_jpeg_dir, train_csv_file, test_jpeg_dir, test_jpeg_additional_dir,\n",
    "                                  img_resize, validation_split_size)\n",
    "preprocessor.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"X_train/y_train length: {}/{}\".format(len(preprocessor.X_train), len(preprocessor.y_train)))\n",
    "print(\"X_val/y_val length: {}/{}\".format(len(preprocessor.X_val), len(preprocessor.y_val)))\n",
    "print(\"X_test/X_test_filename length: {}/{}\".format(len(preprocessor.X_test), len(preprocessor.X_test_filename)))\n",
    "preprocessor.y_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Define model\n",
    "Using Keras, a number of pre-trained models can be download automatically when instantiating a model. They are stored at ~/.keras/models/.\n",
    "\n",
    "Alternatively, get a pre-trained model from  Ocean Protocol, for example, VGG16 is stored at `did:op:9b8791e65b5440049a512a8815495591445919b265654bb3ad7e3b2bdcb4e2bc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.create_model(img_dim=(128, 128, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-tune conv layers\n",
    "We will now finetune all layers in the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "history = History()\n",
    "callbacks = [history,\n",
    "             EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=0, min_lr=1e-7, verbose=1),\n",
    "             ModelCheckpoint(filepath='weights/weights.best.hdf5', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto')]\n",
    "\n",
    "X_train, y_train = preprocessor.X_train, preprocessor.y_train\n",
    "X_val, y_val = preprocessor.X_val, preprocessor.y_val\n",
    "\n",
    "batch_size = 32\n",
    "train_generator = preprocessor.get_train_generator(batch_size)\n",
    "steps = len(X_train) / batch_size\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics = ['accuracy'])# %% [markdown]\n",
    "# ## Section 4: Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "history = model.fit_generator(train_generator, steps, epochs=epochs, verbose=1,\n",
    "                    validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualize Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()# %% [markdown]\n",
    "# ## Section 5: Predict the targets\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load Best Weights from local\n",
    "\n",
    "Alternatively, a pre-trained model can be downloaded from Ocean Protocol. Or, you can publish your own trained model here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(\"weights/weights.best.hdf5\")\n",
    "print(\"Weights loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check Fbeta Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fbeta_score = vgg16.fbeta(model, X_val, y_val)\n",
    "\n",
    "fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions, x_test_filename = vgg16.predict(model, preprocessor, batch_size=32)\n",
    "print(\"Predictions shape: {}\\nFiles name shape: {}\\n1st predictions ({}) entry:\\n{}\".format(predictions.shape,\n",
    "                                                                              x_test_filename.shape,\n",
    "                                                                              x_test_filename[0], predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before mapping our predictions to their appropriate labels we need to figure out what threshold to take for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = [0.2] * len(labels_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now lets map our predictions to their tags by using the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels = vgg16.map_predictions(preprocessor, predictions, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally lets assemble and visualize our predictions for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags_list = [None] * len(predicted_labels)\n",
    "for i, tags in enumerate(predicted_labels):\n",
    "    tags_list[i] = ' '.join(map(str, tags))\n",
    "\n",
    "final_data = [[filename.split(\".\")[0], tags] for filename, tags in zip(x_test_filename, tags_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_data, columns=['image_name', 'tags'])\n",
    "print(\"Predictions rows:\", final_df.size)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_s = pd.Series(list(chain.from_iterable(predicted_labels))).value_counts()\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.barplot(x=tags_s, y=tags_s.index, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If there is a lot of `primary` and `clear` tags, this final dataset may be legit..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
